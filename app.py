# app.py
from __future__ import annotations

import time
import urllib.request
from io import BytesIO
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import streamlit as st
import yfinance as yf

# =========================
# 1. ã‚¢ãƒ—ãƒªè¨­å®š & èªè¨¼
# =========================
st.set_page_config(page_title="äºŒæ®µä¸Šã’ã‚¹ã‚­ãƒ£ãƒŠãƒ¼", layout="wide")
MY_PASSWORD = "stock testa"  # â€»è¦æœ›ã«ã‚ˆã‚Šç›´æ›¸ãã®ã¾ã¾

if "auth" not in st.session_state:
    st.session_state.auth = False

if not st.session_state.auth:
    st.title("ğŸ”’ èªè¨¼")
    pwd = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰", type="password")
    if pwd == MY_PASSWORD:
        st.session_state.auth = True
        st.rerun()
    st.stop()

# =========================
# 2. ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®šï¼ˆæ¡ä»¶æŒ‡å®šï¼‰
# =========================
st.sidebar.title("âš™ï¸ ã‚¹ã‚­ãƒ£ãƒ³æ¡ä»¶")

GITHUB_CSV_URL = "https://raw.githubusercontent.com/watarai0202-netizen/stocktest-app-1/main/data_j.csv"

target_market = st.sidebar.radio("ğŸ“Š å¸‚å ´é¸æŠ", ("ã‚°ãƒ­ãƒ¼ã‚¹", "ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰", "ãƒ—ãƒ©ã‚¤ãƒ "), index=0)

st.sidebar.subheader("ğŸš« ä¸äººæ°—ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
min_avg_value = st.sidebar.slider("æœ€ä½å£²è²·ä»£é‡‘(5æ—¥å¹³å‡/å„„å††)", 0.1, 10.0, 0.5, 0.1)

st.sidebar.subheader("ğŸ“ˆ äºŒæ®µä¸Šã’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆç²¾åº¦UPç‰ˆï¼‰")
jump_days = st.sidebar.selectbox("1. ç¬¬ä¸€æ³¢ã®ç´¯ç©æ—¥æ•°", [2, 3, 4, 5], index=1)
min_jump = st.sidebar.slider(f"2. éå»40æ—¥ã®æœ€å¤§{jump_days}æ—¥ä¸Šæ˜‡ç‡(%)", 10, 80, 20, 1)
vol_dry_limit = st.sidebar.slider("3. å‡ºæ¥é«˜æ¯æ¸‡åº¦ï¼ˆå½“æ—¥/20æ—¥ä¸­å¤®å€¤ï¼‰ä¸Šé™", 0.05, 1.5, 0.55, 0.05)
ma_near_pct = st.sidebar.slider("4. 25æ—¥ç·šã¨ã®ä¹–é›¢(Â±%)", 0.5, 10.0, 2.0, 0.1)
atr_contract_limit = st.sidebar.slider("5. ATRåç¸®ï¼ˆATR5/ATR20ï¼‰ä¸Šé™", 0.3, 1.2, 0.75, 0.05)
dist_to_high_limit = st.sidebar.slider("6. 20æ—¥é«˜å€¤ã¾ã§ã®è·é›¢(%) ä¸Šé™", 0.5, 10.0, 3.0, 0.1)
require_ma_up = st.sidebar.checkbox("7. 25MAãŒä¸Šå‘ãï¼ˆ5æ—¥å‰æ¯”+ï¼‰ã‚’å¿…é ˆ", value=True)

st.sidebar.subheader("ğŸ§ª å®Ÿè¡Œè¨­å®š")
batch_size = st.sidebar.slider("ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆyfinanceä¸€æ‹¬å–å¾—ï¼‰", 10, 100, 50, 5)
use_auto_adjust = st.sidebar.checkbox("ä¾¡æ ¼ã‚’èª¿æ•´ï¼ˆauto_adjust=Trueï¼‰", value=True)
scan_period = st.sidebar.selectbox("ã‚¹ã‚­ãƒ£ãƒ³ç”¨ å–å¾—æœŸé–“", ["3mo", "6mo", "1y"], index=1)

st.sidebar.subheader("ğŸ§ª ä¸Šä½ã ã‘ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ")
enable_backtest = st.sidebar.checkbox("ä¸Šä½å€™è£œã®ã¿ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã™ã‚‹", value=True)
top_n_bt = st.sidebar.slider("ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå¯¾è±¡ï¼ˆã‚¹ã‚³ã‚¢ä¸Šä½Nï¼‰", 1, 80, 20, 1)
bt_period = st.sidebar.selectbox("ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæœŸé–“", ["6mo", "1y", "2y", "5y"], index=2)
bt_horizon = st.sidebar.selectbox("å°†æ¥ã®è©•ä¾¡æœŸé–“ï¼ˆkå–¶æ¥­æ—¥ï¼‰", [3, 5, 10, 15, 20], index=1)
bt_hit_threshold = st.sidebar.slider("å‘½ä¸­åˆ¤å®šï¼ˆkæ—¥å†… æœ€å¤§ä¸Šæ˜‡ãŒ +ä½•% ä»¥ä¸Šï¼‰", 3, 40, 10, 1)

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
if st.sidebar.button("ğŸ”„ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"):
    st.cache_data.clear()
    st.rerun()

# =========================
# 3. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# =========================
@st.cache_data(ttl=3600)
def load_master_data(market_name: str) -> Tuple[List[str], Dict[str, str]]:
    with urllib.request.urlopen(GITHUB_CSV_URL) as resp:
        df = pd.read_csv(BytesIO(resp.read()))

    m_key = f"{market_name}ï¼ˆå†…å›½æ ªå¼ï¼‰"
    df_filtered = df[(df["å¸‚å ´ãƒ»å•†å“åŒºåˆ†"] == m_key) & (df["33æ¥­ç¨®åŒºåˆ†"] != "ï¼")].copy()

    tickers = [f"{str(code).split('.')[0]}.T" for code in df_filtered["ã‚³ãƒ¼ãƒ‰"]]
    info = {f"{str(row['ã‚³ãƒ¼ãƒ‰']).split('.')[0]}.T": row["éŠ˜æŸ„å"] for _, row in df_filtered.iterrows()}
    return tickers, info


def fetch_ohlcv(ticker: str, period: str, auto_adjust: bool) -> pd.DataFrame:
    df = yf.download(
        ticker,
        period=period,
        interval="1d",
        progress=False,
        auto_adjust=auto_adjust,
    )

    if df is None or df.empty:
        return pd.DataFrame()

    # ã¾ã‚Œã« MultiIndex ã§è¿”ã‚‹ã‚±ãƒ¼ã‚¹å¯¾ç­–
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = df.columns.get_level_values(-1)

    df = df.copy()

    # CloseãŒç„¡ã„å ´åˆã€Adj Closeã‚’Closeã¨ã—ã¦ä½¿ã†
    if "Close" not in df.columns:
        if "Adj Close" in df.columns:
            df["Close"] = df["Adj Close"]
        else:
            return pd.DataFrame()

    # dropnaå…¨åˆ—ã¯å±é™ºãªã®ã§Closeã ã‘
    df = df.dropna(subset=["Close"])
    if df.empty:
        return pd.DataFrame()

    # æ¬ ã‘ã‚„ã™ã„åˆ—ã‚’å¿…ãšä½œã‚‹ï¼ˆCloseã§è£œå®Œï¼‰
    for col in ["Open", "High", "Low"]:
        if col not in df.columns:
            df[col] = df["Close"]
        else:
            df[col] = df[col].fillna(df["Close"])

    if "Volume" not in df.columns:
        df["Volume"] = 0
    else:
        df["Volume"] = df["Volume"].fillna(0)

    # å‹ã‚’å®‰å®šåŒ–
    for col in ["Open", "High", "Low", "Close", "Volume"]:
        df[col] = pd.to_numeric(df[col], errors="coerce")

    df = df.dropna(subset=["Close"]).sort_index()
    return df

def compute_atr(df: pd.DataFrame, period: int) -> pd.Series:
    high = df["High"].astype(float)
    low = df["Low"].astype(float)
    close = df["Close"].astype(float)
    prev_close = close.shift(1)

    tr = pd.concat(
        [(high - low).abs(), (high - prev_close).abs(), (low - prev_close).abs()],
        axis=1,
    ).max(axis=1)

    return tr.rolling(period).mean()


def check_strategy(
    data: pd.DataFrame,
    *,
    min_avg_value_: float,
    jump_days_: int,
    min_jump_: float,
    vol_dry_limit_: float,
    ma_near_pct_: float,
    atr_contract_limit_: float,
    dist_to_high_limit_: float,
    require_ma_up_: bool,
) -> Tuple[bool, str, Dict[str, float]]:
    need_len = max(60, 25 + 5, 20 + jump_days_)
    if len(data) < need_len:
        return False, "ãƒ‡ãƒ¼ã‚¿ä¸è¶³", {}

    c = data["Close"].astype(float)
    v = data["Volume"].astype(float)

    # å£²è²·ä»£é‡‘ï¼ˆç›´è¿‘5æ—¥å¹³å‡/å„„å††ï¼‰
    avg_val = (c * v).tail(5).mean() / 1e8
    if avg_val < min_avg_value_:
        return False, "å£²è²·ä»£é‡‘ä¸è¶³", {"avg_val": float(avg_val)}

    # ç¬¬ä¸€æ³¢ï¼šéå»40æ—¥ã§æœ€å¤§Næ—¥ä¸Šæ˜‡ç‡
    jump_series = (c / c.shift(jump_days_) - 1.0) * 100.0
    max_jump = jump_series.tail(40).max()
    if pd.isna(max_jump) or max_jump < min_jump_:
        return False, "ç¬¬ä¸€æ³¢å¼±ã„", {"max_jump": float(max_jump) if pd.notna(max_jump) else 0.0}

    # æ¯æ¸‡ï¼šä¸­å¤®å€¤RVOL
    v_med20 = v.tail(20).median()
    rvol_med = (v.iloc[-1] / v_med20) if v_med20 > 0 else 9.9
    if rvol_med > vol_dry_limit_:
        return False, "æ¯æ¸‡ã—ã¦ãªã„", {"rvol": float(rvol_med)}

    # 25MAä¹–é›¢
    ma25 = c.rolling(25).mean().iloc[-1]
    curr_p = c.iloc[-1]
    diff_ma25 = abs(curr_p - ma25) / ma25 * 100.0
    if diff_ma25 > ma_near_pct_:
        return False, "25MAä¹–é›¢å¤§", {"diff_ma25": float(diff_ma25)}

    # ATRåç¸®
    atr5 = compute_atr(data, 5).iloc[-1]
    atr20 = compute_atr(data, 20).iloc[-1]
    atr_ratio = (atr5 / atr20) if atr20 and atr20 > 0 else 9.9
    if atr_ratio > atr_contract_limit_:
        return False, "ãƒœãƒ©åç¸®å¼±ã„", {"atr_ratio": float(atr_ratio)}

    # é«˜å€¤è·é›¢ï¼ˆ20æ—¥ï¼‰
    high20 = c.tail(20).max()
    dist_to_high = (high20 - curr_p) / curr_p * 100.0
    if dist_to_high > dist_to_high_limit_:
        return False, "é«˜å€¤ã¾ã§é ã„", {"dist_to_high": float(dist_to_high)}

    # MAã®å‘ã
    ma25_now = c.rolling(25).mean().iloc[-1]
    ma25_prev = c.rolling(25).mean().shift(5).iloc[-1]
    ma25_slope = ma25_now - ma25_prev
    if require_ma_up_ and not (ma25_slope > 0):
        return False, "MAä¸‹å‘ã", {"ma25_slope": float(ma25_slope)}

    metrics = {
        "avg_val": float(avg_val),
        "max_jump": float(max_jump),
        "rvol": float(rvol_med),
        "diff_ma25": float(diff_ma25),
        "atr_ratio": float(atr_ratio),
        "dist_to_high": float(dist_to_high),
        "ma25_slope": float(ma25_slope),
        "price": float(curr_p),
    }
    return True, "OK", metrics


def score_metrics(m: Dict[str, float]) -> float:
    max_jump = max(0.0, min(m.get("max_jump", 0.0), 200.0))
    rvol = max(0.01, min(m.get("rvol", 9.9), 9.9))
    atr_ratio = max(0.01, min(m.get("atr_ratio", 9.9), 9.9))
    dist = max(0.0, min(m.get("dist_to_high", 99.0), 99.0))
    diff = max(0.0, min(m.get("diff_ma25", 99.0), 99.0))
    slope = m.get("ma25_slope", 0.0)

    s_jump = max_jump / 80.0
    s_rvol = 1.0 / rvol
    s_atr = 1.0 / atr_ratio
    s_dist = 1.0 / (1.0 + dist)
    s_diff = 1.0 / (1.0 + diff)
    s_slope = 0.15 if slope > 0 else 0.0

    return (
        1.30 * s_jump +
        1.10 * s_rvol +
        1.10 * s_atr +
        0.90 * s_dist +
        0.70 * s_diff +
        s_slope
    )
    
def score_series(
    df: pd.DataFrame,
    *,
    jump_days_: int,
    min_avg_value_floor_: float,
) -> pd.Series:
    c = df["Close"].astype(float)
    v = df["Volume"].astype(float)

    jump = (c / c.shift(jump_days_) - 1.0) * 100.0
    max_jump_40 = jump.rolling(40).max().clip(lower=0)

    v_med20 = v.rolling(20).median()
    rvol_med = (v / v_med20).replace([np.inf, -np.inf], np.nan)

    ma25 = c.rolling(25).mean()
    diff_ma25 = ((c - ma25).abs() / ma25 * 100.0).replace([np.inf, -np.inf], np.nan)

    atr5 = compute_atr(df, 5)
    atr20 = compute_atr(df, 20)
    atr_ratio = (atr5 / atr20).replace([np.inf, -np.inf], np.nan)

    high20 = c.rolling(20).max()
    dist_to_high = ((high20 - c) / c * 100.0).replace([np.inf, -np.inf], np.nan)

    avg_val = (c * v).rolling(5).mean() / 1e8

    s_jump = (max_jump_40 / 80.0).clip(upper=3.0)
    s_rvol = (1.0 / rvol_med.clip(lower=0.05)).clip(upper=10.0)
    s_atr  = (1.0 / atr_ratio.clip(lower=0.20)).clip(upper=10.0)
    s_dist = (1.0 / (1.0 + dist_to_high.clip(lower=0.0))).clip(upper=1.0)
    s_diff = (1.0 / (1.0 + diff_ma25.clip(lower=0.0))).clip(upper=1.0)

    ma25_slope = ma25 - ma25.shift(5)
    s_slope = (ma25_slope > 0).astype(float) * 0.15

    score = (
        1.30 * s_jump +
        1.10 * s_rvol +
        1.10 * s_atr +
        0.90 * s_dist +
        0.70 * s_diff +
        s_slope
    )

    # â˜…ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã§ã¯ â€œåºŠâ€ ã‚’åˆ¥ã«ã™ã‚‹ï¼ˆã“ã“ãŒé‡è¦ï¼‰
    score = score.where(avg_val >= float(min_avg_value_floor_))
    return score



# =========================
# 4. ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼ˆä¸Šä½ã ã‘ï¼‰
# =========================
def compute_signal_series(
    df: pd.DataFrame,
    *,
    min_avg_value_: float,
    jump_days_: int,
    min_jump_: float,
    vol_dry_limit_: float,
    ma_near_pct_: float,
    atr_contract_limit_: float,
    dist_to_high_limit_: float,
    require_ma_up_: bool,
) -> pd.Series:
    df = df.copy()
    c = df["Close"].astype(float)
    v = df["Volume"].astype(float)

    avg_val = (c * v).rolling(5).mean() / 1e8

    jump = (c / c.shift(jump_days_) - 1.0) * 100.0
    max_jump_40 = jump.rolling(40).max()

    v_med20 = v.rolling(20).median()
    rvol_med = v / v_med20

    ma25 = c.rolling(25).mean()
    diff_ma25 = (c - ma25).abs() / ma25 * 100.0

    atr5 = compute_atr(df, 5)
    atr20 = compute_atr(df, 20)
    atr_ratio = atr5 / atr20

    high20 = c.rolling(20).max()
    dist_to_high = (high20 - c) / c * 100.0

    ma25_slope = ma25 - ma25.shift(5)

    cond = (
        (avg_val >= min_avg_value_) &
        (max_jump_40 >= min_jump_) &
        (rvol_med <= vol_dry_limit_) &
        (diff_ma25 <= ma_near_pct_) &
        (atr_ratio <= atr_contract_limit_) &
        (dist_to_high <= dist_to_high_limit_)
    )
    if require_ma_up_:
        cond = cond & (ma25_slope > 0)

    return cond.fillna(False)


def backtest_one(df: pd.DataFrame, signal: pd.Series, horizon: int) -> pd.DataFrame:
    df = df.copy()

    # signal ã‚’ df ã® index ã«åˆã‚ã›ã‚‹ï¼ˆã“ã“ãŒã‚ºãƒ¬ã‚‹ã¨ KeyError ã®æ¸©åºŠï¼‰
    signal = signal.reindex(df.index).fillna(False)

    c = df["Close"].astype(float).to_numpy()
    h = df["High"].astype(float).to_numpy()
    l = df["Low"].astype(float).to_numpy()
    sig = signal.to_numpy()

    idxs = np.where(sig)[0]
    rows = []
    n = len(df)

    for i in idxs:
        if i + 1 >= n:
            continue
        end = min(n, i + 1 + horizon)

        base = c[i]
        if not np.isfinite(base) or base <= 0:
            continue

        max_high = np.nanmax(h[i + 1:end])
        min_low = np.nanmin(l[i + 1:end])

        max_up = (max_high / base - 1.0) * 100.0 if np.isfinite(max_high) else np.nan
        max_dd = (min_low / base - 1.0) * 100.0 if np.isfinite(min_low) else np.nan

        rows.append({"date": df.index[i], "base_close": base, "max_up_%": max_up, "max_dd_%": max_dd})

    return pd.DataFrame(rows)


@st.cache_data(ttl=3600)
def run_backtest_top(
    tickers: List[str],
    *,
    period: str,
    auto_adjust: bool,
    min_avg_value_: float,
    jump_days_: int,
    min_jump_: float,
    vol_dry_limit_: float,
    ma_near_pct_: float,
    atr_contract_limit_: float,
    dist_to_high_limit_: float,
    require_ma_up_: bool,
    horizon: int,
    hit_threshold: float,
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    summaries = []
    all_trades = []

    for t in tickers:
        df = fetch_ohlcv(t, period=period, auto_adjust=auto_adjust)
        if df.empty:
            summaries.append(
                {
                    "ticker": t,
                    "signals": 0,
                    "hit_rate_%": np.nan,
                    f"avg_max_up_{horizon}d_%": np.nan,
                    f"med_max_up_{horizon}d_%": np.nan,
                    f"worst_dd_{horizon}d_%": np.nan,
                }
            )
            continue

     # æ—§: æ¡ä»¶ANDã®å®Œå…¨ä¸€è‡´ã‚·ã‚°ãƒŠãƒ«
# sig = compute_signal_series(...)

# æ–°: ã‚¹ã‚³ã‚¢ä¸Šä½5%ã®æ—¥ã‚’ã‚·ã‚°ãƒŠãƒ«æ‰±ã„
score = score_series(df)          # å„æ—¥ã®ã‚¹ã‚³ã‚¢ï¼ˆé€£ç¶šå€¤ï¼‰
score_valid = score.dropna()      # æœ‰åŠ¹ãªæ—¥ã ã‘ã§é–¾å€¤è¨ˆç®—

if score_valid.empty:
    # ãƒ‡ãƒ¼ã‚¿ä¸è¶³ãªã©ã§ã‚¹ã‚³ã‚¢ãŒä½œã‚Œãªã„å ´åˆ
    sig = pd.Series(False, index=df.index)
else:
    thr = score_valid.quantile(0.95)   # ä¸Šä½5% = 95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«
    sig = (score >= thr).fillna(False) # ã‚·ã‚°ãƒŠãƒ«ï¼ˆTrue/Falseï¼‰


        trades = backtest_one(df, sig, horizon=horizon)
        if trades.empty:
            summaries.append(
                {
                    "ticker": t,
                    "signals": 0,
                    "hit_rate_%": np.nan,
                    f"avg_max_up_{horizon}d_%": np.nan,
                    f"med_max_up_{horizon}d_%": np.nan,
                    f"worst_dd_{horizon}d_%": np.nan,
                }
            )
            continue

        hit = (trades["max_up_%"] >= hit_threshold).mean() * 100.0
        avg_up = float(np.nanmean(trades["max_up_%"]))
        med_up = float(np.nanmedian(trades["max_up_%"]))
        worst_dd = float(np.nanmin(trades["max_dd_%"]))

        summaries.append(
            {
                "ticker": t,
                "signals": int(len(trades)),
                "hit_rate_%": float(hit),
                f"avg_max_up_{horizon}d_%": avg_up,
                f"med_max_up_{horizon}d_%": med_up,
                f"worst_dd_{horizon}d_%": worst_dd,
            }
        )

        trades_out = trades.copy()
        trades_out["ticker"] = t
        all_trades.append(trades_out)

    sum_df = pd.DataFrame(summaries)
    trades_df = pd.concat(all_trades, ignore_index=True) if all_trades else pd.DataFrame()
    return sum_df, trades_df


# =========================
# 5. ãƒ¡ã‚¤ãƒ³ç”»é¢
# =========================
st.title(f"ğŸš€ {target_market}ãƒ»äºŒæ®µä¸Šã’ç‹™ã„ï¼ˆç²¾åº¦UPç‰ˆï¼‰")
st.caption("ç¬¬ä¸€æ³¢ï¼ˆè¤‡æ•°æ—¥ä¸Šæ˜‡ï¼‰â†’æ¯æ¸‡ï¼ˆä¸­å¤®å€¤RVOLï¼‰â†’25MAä»˜è¿‘â†’ATRåç¸®â†’é«˜å€¤ãŒè¿‘ã„ã€ã§â€œçŸ­æœŸå†å™´ç«â€å€™è£œã‚’å„ªå…ˆã€‚")

colA, colB, colC = st.columns([1.1, 1.1, 1.6])
with colA:
    st.write("**ã‚¹ã‚­ãƒ£ãƒ³å¯¾è±¡**")
    st.write(f"- å¸‚å ´: {target_market}")
    st.write(f"- æœŸé–“: {scan_period} / 1d")

with colB:
    st.write("**ä¸»è¦æ¡ä»¶**")
    st.write(f"- å£²è²·ä»£é‡‘: {min_avg_value:.2f}å„„/æ—¥ä»¥ä¸Š")
    st.write(f"- ç¬¬ä¸€æ³¢: {jump_days}æ—¥ã§{min_jump:.0f}%ä»¥ä¸Š")
    st.write(f"- æ¯æ¸‡: RVOLâ‰¤{vol_dry_limit:.2f}")

with colC:
    st.write("**ãƒˆãƒªã‚¬ãƒ¼å¯„ã›**")
    st.write(f"- 25MAä¹–é›¢â‰¤{ma_near_pct:.1f}% / ATR5/20â‰¤{atr_contract_limit:.2f} / é«˜å€¤è·é›¢â‰¤{dist_to_high_limit:.1f}%")
    st.write(f"- 25MAä¸Šå‘ãå¿…é ˆ: {'ON' if require_ma_up else 'OFF'}")

if st.button("ğŸ“¡ ã‚¹ã‚­ãƒ£ãƒ³é–‹å§‹", type="primary"):
    tickers, info_db = load_master_data(target_market)
    if not tickers:
        st.warning("å¯¾è±¡éŠ˜æŸ„ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        st.stop()

    results: List[Dict[str, object]] = []
    fail_reasons: Dict[str, int] = {}
    fetch_fail: List[str] = []

    progress_bar = st.progress(0)
    status_text = st.empty()

    total = len(tickers)
    t0 = time.time()

    for i in range(0, total, batch_size):
        batch = tickers[i: i + batch_size]
        status_text.text(f"ã‚¹ã‚­ãƒ£ãƒ³ä¸­... {i}/{total}")
        progress_bar.progress(min(1.0, i / total))

        try:
            df_batch = yf.download(
                batch,
                period=scan_period,
                interval="1d",
                progress=False,
                group_by="ticker",
                threads=True,
                auto_adjust=use_auto_adjust,
            )
            if not isinstance(df_batch.columns, pd.MultiIndex):
                df_batch = pd.concat({batch[0]: df_batch}, axis=1)

            # MultiIndex: level0=ticker
            tickers_in_batch = set(df_batch.columns.get_level_values(0))

            for t in batch:
                if t not in tickers_in_batch:
                    fetch_fail.append(t)
                    continue

                stock_data = df_batch[t].dropna()
                # å¿µã®ãŸã‚æœ€ä½åˆ—ãƒã‚§ãƒƒã‚¯
                need_cols = {"Open", "High", "Low", "Close", "Volume"}
                if stock_data.empty or not need_cols.issubset(set(stock_data.columns)):
                    fetch_fail.append(t)
                    continue

                ok, reason, m = check_strategy(
                    stock_data,
                    min_avg_value_=min_avg_value,
                    jump_days_=jump_days,
                    min_jump_=min_jump,
                    vol_dry_limit_=vol_dry_limit,
                    ma_near_pct_=ma_near_pct,
                    atr_contract_limit_=atr_contract_limit,
                    dist_to_high_limit_=dist_to_high_limit,
                    require_ma_up_=require_ma_up,
                )
                if not ok:
                    fail_reasons[reason] = fail_reasons.get(reason, 0) + 1
                    continue

                sc = score_metrics(m)
                results.append(
                    {
                        "ticker": t,
                        "ã‚³ãƒ¼ãƒ‰": t.replace(".T", ""),
                        "éŠ˜æŸ„å": info_db.get(t, "ä¸æ˜"),
                        "ã‚¹ã‚³ã‚¢": float(sc),
                        "ç¾åœ¨å€¤": float(m["price"]),
                        f"ç¬¬ä¸€æ³¢({jump_days}æ—¥)%": float(m["max_jump"]),
                        "æ¯æ¸‡RVOL(ä¸­å¤®å€¤)": float(m["rvol"]),
                        "25MAä¹–é›¢%": float(m["diff_ma25"]),
                        "ATR5/20": float(m["atr_ratio"]),
                        "é«˜å€¤è·é›¢%": float(m["dist_to_high"]),
                        "ä»£é‡‘(å„„å††)": float(m["avg_val"]),
                    }
                )
        except Exception:
            fetch_fail.extend(batch)
            continue

    progress_bar.progress(1.0)
    status_text.empty()
    elapsed = time.time() - t0

    # ã‚µãƒãƒªãƒ¼
    st.subheader("çµæœã‚µãƒãƒªãƒ¼")
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("ãƒ’ãƒƒãƒˆéŠ˜æŸ„æ•°", f"{len(results)}")
    c2.metric("å¯¾è±¡éŠ˜æŸ„æ•°", f"{total}")
    c3.metric("å–å¾—å¤±æ•—æ•°", f"{len(fetch_fail)}")
    c4.metric("å‡¦ç†æ™‚é–“(ç§’)", f"{elapsed:.1f}")

    if fail_reasons:
        with st.expander("è½é¸ç†ç”±ã®å†…è¨³ï¼ˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ï¼‰", expanded=False):
            reason_df = pd.DataFrame(
                [{"ç†ç”±": k, "ä»¶æ•°": v} for k, v in sorted(fail_reasons.items(), key=lambda x: -x[1])]
            )
            st.dataframe(reason_df, use_container_width=True, hide_index=True)

    if fetch_fail:
        with st.expander("å–å¾—å¤±æ•—ãƒ†ã‚£ãƒƒã‚«ãƒ¼ï¼ˆyfinanceæ¬ æãªã©ï¼‰", expanded=False):
            st.write(", ".join(fetch_fail[:300]) + (" ..." if len(fetch_fail) > 300 else ""))

    if not results:
        st.warning("è©²å½“éŠ˜æŸ„ãªã—ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç·©ã‚ã¦ã¿ã¦ãã ã•ã„ã€‚")
        st.stop()

    # è¡¨ç¤º
    st.success(f"ğŸ¯ {len(results)} éŠ˜æŸ„ãŒæ¡ä»¶ã«åˆè‡´ã—ã¾ã—ãŸï¼ˆã‚¹ã‚³ã‚¢é †ï¼‰")
    res_df = pd.DataFrame(results).sort_values("ã‚¹ã‚³ã‚¢", ascending=False).reset_index(drop=True)

    show_df = res_df.drop(columns=["ticker"]).copy()
    show_df["ã‚¹ã‚³ã‚¢"] = show_df["ã‚¹ã‚³ã‚¢"].map(lambda x: f"{x:.3f}")
    show_df["ç¾åœ¨å€¤"] = show_df["ç¾åœ¨å€¤"].map(lambda x: f"{x:,.1f}")
    show_df[f"ç¬¬ä¸€æ³¢({jump_days}æ—¥)%"] = show_df[f"ç¬¬ä¸€æ³¢({jump_days}æ—¥)%"].map(lambda x: f"{x:.1f}%")
    show_df["æ¯æ¸‡RVOL(ä¸­å¤®å€¤)"] = show_df["æ¯æ¸‡RVOL(ä¸­å¤®å€¤)"].map(lambda x: f"{x:.2f}å€")
    show_df["25MAä¹–é›¢%"] = show_df["25MAä¹–é›¢%"].map(lambda x: f"{x:.1f}%")
    show_df["ATR5/20"] = show_df["ATR5/20"].map(lambda x: f"{x:.2f}")
    show_df["é«˜å€¤è·é›¢%"] = show_df["é«˜å€¤è·é›¢%"].map(lambda x: f"{x:.1f}%")
    show_df["ä»£é‡‘(å„„å††)"] = show_df["ä»£é‡‘(å„„å††)"].map(lambda x: f"{x:.2f}å„„å††")

    st.dataframe(show_df, use_container_width=True, hide_index=True)

    # =========================
    # ä¸Šä½ã ã‘ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
    # =========================
    if enable_backtest:
        st.subheader("ğŸ§ª ä¸Šä½å€™è£œã®ã¿ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼ˆåŒæ¡ä»¶ã‚·ã‚°ãƒŠãƒ«â†’å°†æ¥kæ—¥ï¼‰")

        top_n = min(int(top_n_bt), len(res_df))
        top_tickers = res_df.head(top_n)["ticker"].tolist()

        st.write(
            f"- å¯¾è±¡ï¼šã‚¹ã‚³ã‚¢ä¸Šä½ **{top_n}** éŠ˜æŸ„  / æœŸé–“ï¼š**{bt_period}**  / å…ˆèª­ã¿ï¼š**{bt_horizon}** æ—¥  / å‘½ä¸­ï¼š**+{bt_hit_threshold}%** ä»¥ä¸Š"
        )

        with st.spinner("ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆè¨ˆç®—ä¸­ï¼ˆä¸Šä½ã ã‘ï¼‰..."):
            sum_df, trades_df = run_backtest_top(
                top_tickers,
                period=bt_period,
                auto_adjust=use_auto_adjust,
                min_avg_value_=min_avg_value,
                jump_days_=jump_days,
                min_jump_=min_jump,
                vol_dry_limit_=vol_dry_limit,
                ma_near_pct_=ma_near_pct,
                atr_contract_limit_=atr_contract_limit,
                dist_to_high_limit_=dist_to_high_limit,
                require_ma_up_=require_ma_up,
                horizon=int(bt_horizon),
                hit_threshold=float(bt_hit_threshold),
            )

        if sum_df.empty:
            st.warning("ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆå–å¾—å¤±æ•—/ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®å¯èƒ½æ€§ï¼‰ã€‚")
        else:
            sum_df = sum_df.copy()
            sum_df["ã‚³ãƒ¼ãƒ‰"] = sum_df["ticker"].str.replace(".T", "", regex=False)
            sum_df["éŠ˜æŸ„å"] = sum_df["ticker"].map(lambda x: info_db.get(x, "ä¸æ˜"))

            hit_col = "hit_rate_%"
            med_col = f"med_max_up_{bt_horizon}d_%"
            sig_col = "signals"
            sum_df = sum_df.sort_values([hit_col, med_col, sig_col], ascending=[False, False, False])

            show_bt = sum_df[
                ["ã‚³ãƒ¼ãƒ‰", "éŠ˜æŸ„å", "signals", "hit_rate_%",
                 f"avg_max_up_{bt_horizon}d_%", f"med_max_up_{bt_horizon}d_%", f"worst_dd_{bt_horizon}d_%"]
            ].copy()

            show_bt["hit_rate_%"] = show_bt["hit_rate_%"].map(lambda x: "-" if pd.isna(x) else f"{x:.1f}%")
            show_bt[f"avg_max_up_{bt_horizon}d_%"] = show_bt[f"avg_max_up_{bt_horizon}d_%"].map(lambda x: "-" if pd.isna(x) else f"{x:.1f}%")
            show_bt[f"med_max_up_{bt_horizon}d_%"] = show_bt[f"med_max_up_{bt_horizon}d_%"].map(lambda x: "-" if pd.isna(x) else f"{x:.1f}%")
            show_bt[f"worst_dd_{bt_horizon}d_%"] = show_bt[f"worst_dd_{bt_horizon}d_%"].map(lambda x: "-" if pd.isna(x) else f"{x:.1f}%")

            st.write("**éŠ˜æŸ„åˆ¥ã‚µãƒãƒªãƒ¼ï¼ˆæ¡ä»¶ãŒå‡ºãŸæ—¥ã®ã€ãã®å¾Œkæ—¥å†…ã®æˆç¸¾ï¼‰**")
            st.dataframe(show_bt, use_container_width=True, hide_index=True)

            if not trades_df.empty:
                all_hit = (trades_df["max_up_%"] >= float(bt_hit_threshold)).mean() * 100.0
                all_med = float(np.nanmedian(trades_df["max_up_%"]))
                all_worst = float(np.nanmin(trades_df["max_dd_%"]))

                m1, m2, m3, m4 = st.columns(4)
                m1.metric("å…¨ã‚·ã‚°ãƒŠãƒ«æ•°", f"{len(trades_df)}")
                m2.metric("å…¨ä½“ å‘½ä¸­ç‡", f"{all_hit:.1f}%")
                m3.metric("å…¨ä½“ ä¸­å¤®å€¤(MaxUp)", f"{all_med:.1f}%")
                m4.metric("å…¨ä½“ ãƒ¯ãƒ¼ã‚¹ãƒˆDD", f"{all_worst:.1f}%")

                with st.expander("å…¨ã‚·ã‚°ãƒŠãƒ«æ˜ç´°ï¼ˆå¿…è¦ãªã‚‰ï¼‰", expanded=False):
                    td = trades_df.copy()
                    td["ã‚³ãƒ¼ãƒ‰"] = td["ticker"].str.replace(".T", "", regex=False)
                    td["éŠ˜æŸ„å"] = td["ticker"].map(lambda x: info_db.get(x, "ä¸æ˜"))
                    td = td[["date", "ã‚³ãƒ¼ãƒ‰", "éŠ˜æŸ„å", "base_close", "max_up_%", "max_dd_%"]]
                    td["base_close"] = td["base_close"].map(lambda x: f"{x:,.1f}")
                    td["max_up_%"] = td["max_up_%"].map(lambda x: f"{x:.1f}%")
                    td["max_dd_%"] = td["max_dd_%"].map(lambda x: f"{x:.1f}%")
                    st.dataframe(td, use_container_width=True, hide_index=True)

    # ãƒãƒ£ãƒ¼ãƒˆç¢ºèªå°ç·š
    st.subheader("å€™è£œãƒãƒ£ãƒ¼ãƒˆï¼ˆãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ç¢ºèªï¼‰")
    pick_code = st.selectbox("éŠ˜æŸ„ã‚’é¸æŠ", options=res_df["ã‚³ãƒ¼ãƒ‰"].tolist(), index=0)
    pick_ticker = f"{pick_code}.T"

    try:
        df_one = yf.download(
            pick_ticker,
            period=scan_period,
            interval="1d",
            progress=False,
            auto_adjust=use_auto_adjust,
        ).dropna()

        if len(df_one) >= 10 and "Close" in df_one.columns:
            st.write(f"**{pick_code}ï¼š{info_db.get(pick_ticker, 'ä¸æ˜')}**")
            st.line_chart(df_one["Close"], height=260)
            if "Volume" in df_one.columns:
                st.bar_chart(df_one["Volume"], height=180)
        else:
            st.info("ãƒãƒ£ãƒ¼ãƒˆè¡¨ç¤ºã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    except Exception as e:
        st.warning(f"ãƒãƒ£ãƒ¼ãƒˆå–å¾—ã«å¤±æ•—: {e}")

else:
    st.info("å·¦ã®æ¡ä»¶ã‚’èª¿æ•´ã—ã¦ã€ŒğŸ“¡ ã‚¹ã‚­ãƒ£ãƒ³é–‹å§‹ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
